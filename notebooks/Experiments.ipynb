{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85d6169",
   "metadata": {},
   "source": [
    "# OCR Correction in Dutch Government Documents \n",
    "**Code for the TPDL 2023 Short paper \"Making PDFs Accessible for Visually Impaired\n",
    "Users (and Findable for Everybody Else)\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b67c40",
   "metadata": {},
   "source": [
    "In this notebook we present the code used to obtain the results we presented in the TPDL short paper currently under review. We will be testing a simple method to detect bad segments in OCRed text of released government documents using wordlist membership, which we will be using to compare the quality of the original text in the PDF (extracted using pdftotext) and text extracted using Tesseract, and see whether the usage of Tesseract increases the overall quality. We then also use ChatGPT to correct the texts and evaluate the effectiveness of this measure.\n",
    "\n",
    "**Note**: In line with OpenAI policies we do not share our private API token, so you will have to request your own, the free OpenAI token that grants a 5 dollor budget should be more than enough to run the examples that we did.\n",
    "\n",
    "## Index\n",
    "1. [Setting Up](#setup)\n",
    "2. [Word List Based Detection](#wordlists)\n",
    "3. [PDFToText and Tesseract Comparison](#comparison)\n",
    "3. [ChatGPT](#chatgpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c826f0",
   "metadata": {},
   "source": [
    "<h1><a id=\"setup\"> Setting Things Up</a></h1>\n",
    "\n",
    "First we are going to detect bad segments based on a list of Dutch words. We consider segments bad if they contain words that are not in the word list, but we also consider segments bad if there are at most N consecutive good words in the segments, where N is a parameter. We update the wordlist with words from the ParlaMint dataset as this is also from the political domain, and might give us some more domain specific words. All the required files are present in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0c9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in parlamint\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from cleantext import clean\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# We also load a preprocessing file, which contains functions for getting the updated word list,\n",
    "# the full tokenization/cleaning pipeline as well as several other functions.\n",
    "from scripts import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9656a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word list file that has been updated with ParlaMint words (using gzip)\n",
    "with gzip.open('../data/wordlist.txt.gz', 'r') as f:\n",
    "        dutch_wordlist = set([item.strip() for item in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1285089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word list (with parlamint added) contains 538039 unique words\n"
     ]
    }
   ],
   "source": [
    "print(\"The word list (with parlamint added) contains %d unique words\" % len(dutch_wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db387061",
   "metadata": {},
   "source": [
    "The above wordlist has been constructed as a combination from the wordlist from the ParlaMint dataset and the wordlist from the TaalUnie. We will now also do a small comparison to show the differences between the wordlists, and how much we added by adding the word list from ParlaMint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1062a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the parlamint wordlist and the wordlist without parlamint seperately.\n",
    "with gzip.open('../data/taalunie_wordlist.txt.gz', 'r') as f:\n",
    "        dutch_wordlist_without_parlamint = set([item.strip().lower() for item in f.readlines()])\n",
    "        \n",
    "with gzip.open('../data/parlamint_wordlist.txt.gz', 'r') as f:\n",
    "        parlamint_wordlist = set([item.strip().lower() for item in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac60c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TaalUnie word list has a total of 409555 unique words\n",
      "The Parlamint dataset has a total of 239159 unique words\n",
      "Together both word list contain 648714 words\n",
      "The general wordlist and the parlamint word list have 109809 words in common\n",
      "Taalunie has 299746 words not occurring in the ParlaMint dataset\n",
      "ParlaMint has 129350 words not occurring in the TaalUnie dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"The TaalUnie word list has a total of %d unique words\" % len(dutch_wordlist_without_parlamint))\n",
    "print(\"The Parlamint dataset has a total of %d unique words\" % len(parlamint_wordlist))\n",
    "print(\"Together both word list contain %d words\" % (len(dutch_wordlist_without_parlamint)+len(parlamint_wordlist)))\n",
    "print(\"The general wordlist and the parlamint word list have %d words in common\" % len(dutch_wordlist_without_parlamint & parlamint_wordlist))\n",
    "print(\"Taalunie has %d words not occurring in the ParlaMint dataset\" % len(dutch_wordlist_without_parlamint-parlamint_wordlist))\n",
    "print(\"ParlaMint has %d words not occurring in the TaalUnie dataset\" % len(parlamint_wordlist-dutch_wordlist_without_parlamint))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e9d30",
   "metadata": {},
   "source": [
    "### Loading in the dataset\n",
    "\n",
    "We load in the text from all the pages that are part of besluiten/decisions and apply some filtering to remove pages that have been missclassified as being part of a besluit. The final version is available in the github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8330811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dataframe = pd.read_csv('../data/data.csv.gz')\n",
    "# set the index properly so its unique for every page\n",
    "experiment_dataframe['DOCUMENT_ID'] = experiment_dataframe['ID'].str.split('.').str[:-1].str.join(sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0d26a",
   "metadata": {},
   "source": [
    "<h1><a id=\"wordlists\">Word List Membership Detection</a></h1>\n",
    "\n",
    "Now that we have an updated word list we will be using this to detect bad passages in the besluiten text. We will be doing this for both the Tesseract and pdftotext types and makes comparisons, as well as playing with the tolerance threshold on good words, to see the impact of this threshold. Let's code up the rules in numpy first, then compare threshold, and then compare the two ocr types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a8146ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BadSegmentDetector:\n",
    "    def __init__(self, word_list_path: str = '../data/wordlist.txt.gz'):\n",
    "        # Read in the word list\n",
    "        with gzip.open(word_list_path, 'r') as f:\n",
    "            all_dutch_words = set([item.decode().strip().lower() for item in f])\n",
    "        # remove any numbers from the word list, as we have also remove them from the page tokens.\n",
    "        self.word_list = set([item for item in all_dutch_words if not any(char.isdigit() for char in item)])\n",
    "        print(list(self.word_list)[:10])\n",
    "\n",
    "    def wordlist_membership(self, list_of_tokens: List[str]) -> List[bool]:\n",
    "        # simply check whether a token appears in the word list or not\n",
    "        return np.array([token in self.word_list for token in list_of_tokens])\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(input_string: str, clean_input: bool = True) -> List[str]:\n",
    "        \"\"\"\n",
    "        :param input_string: string containing the text to be preprocessed\n",
    "        :param clean_input: boolean specifying whether to clean the input, \n",
    "        if this is Fale then only tokenization is performed.\n",
    "        Function that handles the preprocessing of input text performs two steps\n",
    "        1. Normalization by removing non-unicode characters and normalizing whitespace\n",
    "        2. Tokenization using nltk\n",
    "        :returns: A list of tokens\n",
    "        \"\"\"\n",
    "        # perform normalization and tokenization\n",
    "        if clean_input:\n",
    "            input_string = clean_func(input_string)\n",
    "        tokenized_text = word_tokenize(input_string)\n",
    "        return tokenized_text\n",
    "        \n",
    "    def detect_bad_segments(self, input_text: str, tolerance=3, clean_input: bool=True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function that detects bad segment in a list of input tokens based on the \n",
    "        word list.\n",
    "        :param inpuput_text: string that contains the input text for which we \n",
    "        calculate the bad segments\n",
    "        :param tolerance: integer specifying the tolerance of good words within a \n",
    "        pad segment\n",
    "        :param clean_input: boolean specifying whether to clean the input, \n",
    "        if this is Fale then only tokenization is performed.\n",
    "        :returns: numpy array with a boolean vector for each token indication whether it is \n",
    "        part of a bad segment or not.\n",
    "        \"\"\"\n",
    "        # Get the input tokens from the text\n",
    "        tokenized_sample = self.preprocess(input_text, clean_input=clean_input)\n",
    "        # Get a boolean vector for each word in the in text indicating whether\n",
    "        # it is in the wordlist or not.\n",
    "        word_list_array = self.wordlist_membership(tokenized_sample)\n",
    "        # Get the words that are in the wordlist\n",
    "        wordlist_members = word_list_array.nonzero()[0]\n",
    "        # Get the start of good segment\n",
    "        good_prefixes = np.diff(wordlist_members, prepend=-2)\n",
    "        good_prefixes = wordlist_members[good_prefixes > 1]\n",
    "\n",
    "        # Check whether a good segment is within the tolerance\n",
    "        # if it is its part of a bad segment, otherwise it is ok.\n",
    "        for item in good_prefixes:\n",
    "            sequence = word_list_array[item: item+tolerance]\n",
    "            if not np.all(sequence):\n",
    "                word_list_array[item:item+tolerance-1] = False\n",
    "\n",
    "        return np.logical_not(word_list_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10107cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coenders', 'wipneus', 'nietverdienende', 'middelmatigste', 'zwanikken', 'toekans', 'kernwapendebatten', 'subsidiestromen', 'collegagelukzoekers', 'schrielere']\n"
     ]
    }
   ],
   "source": [
    "# set up the segmenter for some examples\n",
    "wordlist_segmenter = BadSegmentDetector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badcc6a4",
   "metadata": {},
   "source": [
    "We will test the segmenter on a sample from the dataet, highlightin bad segments in the text with light blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "814ebbbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML, display, Markdown\n\u001b[1;32m      2\u001b[0m sample_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2014\u001b[39m\n\u001b[0;32m----> 3\u001b[0m sample_page_output \u001b[38;5;241m=\u001b[39m \u001b[43mwordlist_segmenter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_bad_segments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_dataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m page_tokens \u001b[38;5;241m=\u001b[39m wordlist_segmenter\u001b[38;5;241m.\u001b[39mpreprocess(experiment_dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[sample_index])\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mBadSegmentDetector.detect_bad_segments\u001b[0;34m(self, input_text, tolerance, clean_input)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mFunction that detects bad segment in a list of input tokens based on the \u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mword list.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mpart of a bad segment or not.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Get the input tokens from the text\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m tokenized_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Get a boolean vector for each word in the in text indicating whether\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# it is in the wordlist or not.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m word_list_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwordlist_membership(tokenized_sample)\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mBadSegmentDetector.preprocess\u001b[0;34m(input_string, clean_input)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# perform normalization and tokenization\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_input:\n\u001b[0;32m---> 28\u001b[0m     input_string \u001b[38;5;241m=\u001b[39m \u001b[43mclean_func\u001b[49m(input_string)\n\u001b[1;32m     29\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m word_tokenize(input_string)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_func' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML, display, Markdown\n",
    "sample_index = 2014\n",
    "sample_page_output = wordlist_segmenter.detect_bad_segments(experiment_dataframe['original'].iloc[sample_index])\n",
    "page_tokens = wordlist_segmenter.preprocess(experiment_dataframe['original'].iloc[sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed86d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the bad segments with colored markup\n",
    "display(Markdown(\" \".join(\"<p style='background-color:powderblue;display: inline-block;'>%s</p>\" % page_tokens[i] if sample_page_output[i] else page_tokens[i] for i in range(len(page_tokens)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29936fc",
   "metadata": {},
   "source": [
    "<h1><a id=\"comparison\">Comparing the Original Text with Tesseract</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b66b92",
   "metadata": {},
   "source": [
    "In this part we will be comparing the quality of the original text with the output of the Tesseract model by comparing the statistics of the detected bad segments by our simple method. We will make distribution plots for a few of the statistics, namely the `number of bad segments`, the `bad ratio` and the `mean bad segment length`. Here we run with our default of 3 good tokens tolerance, which we will also use throughout the rest of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6226d46f",
   "metadata": {},
   "source": [
    "We have to be a bit careful here, as especially the original text has quite a lot of pages where text is missing, and this will influence the numbers that we take. We will add the information to the entire dataframe, but when making the results we will focus only on cases where both versions actually have text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae828e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The original text has %d empty pages\" % experiment_dataframe['original'].isna().sum())\n",
    "print(\"Tesseract has %d empty pages\" % experiment_dataframe['tesseract'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79690e4",
   "metadata": {},
   "source": [
    "Below we will write a function that will calculate the bad segments for every page in our datasets, and will also calculate various statistics that we can then use to obtain the stats over both versions of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First some code to get general stats on all text (or a selection)\n",
    "def run_bad_segment_detection(input_dataframe: list, tolerance: int):\n",
    "    \"\"\"\n",
    "    Function that runs that bad segment detection on a dataframe and returns\n",
    "    general statistics on the bad segments in the text.\n",
    "    \"\"\"\n",
    "    det = BadSegmentDetector()\n",
    "    all_bad_segment_lengths = []\n",
    "    all_segment_indices = []\n",
    "    results = dict()\n",
    "    for document_id, document in tqdm(input_dataframe.to_dict().items()):\n",
    "        bad_segments = det.detect_bad_segments(document, tolerance=tolerance, clean_input=False)\n",
    "        # skips empty pages\n",
    "        if True in bad_segments:\n",
    "            bad_segment_length = np.diff(np.where(np.concatenate(([bad_segments[0]],\n",
    "                         bad_segments[:-1] != bad_segments[1:],\n",
    "                         [True])))[0])[::2]\n",
    "            num_bad_segments = len(bad_segment_length)\n",
    "            all_bad_segment_lengths.extend(bad_segment_length)\n",
    "        else:\n",
    "            num_bad_segments = 0\n",
    "            bad_segment_length = np.array([0])\n",
    "\n",
    "        starts = np.nonzero(np.diff(bad_segments.astype(int)) == 1)[0]+1\n",
    "        stops = np.nonzero(np.diff(bad_segments.astype(int)) == -1)[0]+1\n",
    "        \n",
    "        if bad_segments[0]:\n",
    "            starts = np.insert(starts, 0, 0)\n",
    "        if bad_segments[-1]:\n",
    "            stops = np.append(stops, len(bad_segments))\n",
    "            \n",
    "        segment_indices = list(zip(starts, stops))\n",
    "        all_segment_indices.append(segment_indices)\n",
    "\n",
    "        mean_bad_segment_length = bad_segment_length.mean()\n",
    "        num_bad_tokens = bad_segments.sum().astype(int)\n",
    "        shortest_bad_segment, longest_bad_segment = bad_segment_length.min().astype(int), bad_segment_length.max().astype(int)\n",
    "        results[document_id] = {'Number of bad Segments': num_bad_segments,\n",
    "                            'Mean Bad Segment Length': mean_bad_segment_length,\n",
    "                            'Max Bad Segment Length': longest_bad_segment,\n",
    "                            'Min Bad Segment Length': shortest_bad_segment,\n",
    "                            'Number of bad Tokens': num_bad_tokens,\n",
    "                            'Bad Ratio': bad_segments.mean(),\n",
    "                            'Num Tokens': len(bad_segments)}\n",
    "        \n",
    "    return pd.DataFrame.from_dict(results, orient='index'), all_bad_segment_lengths, all_segment_indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913615a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the algorithm on both versions.\n",
    "# We also get the length of all bad segments, and return this seperately as it is one a segment level, not a page level\n",
    "pages_with_ocr = experiment_dataframe.dropna(subset=['original', 'tesseract'])\n",
    "pdftotext_results, all_seg_lengths_pdf, pdftotext_indices = run_bad_segment_detection(pages_with_ocr['original'], tolerance=3)\n",
    "tesseract_results, all_seg_lengths_tes, tesseract_indices = run_bad_segment_detection(pages_with_ocr['tesseract'], tolerance=3)\n",
    "\n",
    "pdftotext_results['ID'] = pages_with_ocr['ID']\n",
    "tesseract_results['ID'] = pages_with_ocr['ID']\n",
    "pdftotext_results['publisher'] = pages_with_ocr['publisher']\n",
    "tesseract_results['publisher'] = pages_with_ocr['publisher']\n",
    "# Easiest thing is to make some distribution plots of the stats we got here between tesseract and pdftotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdftotext_results.drop(['ID', 'publisher'], axis=1).describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseract_results.drop(['ID', 'publisher'], axis=1).describe().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c85f97b",
   "metadata": {},
   "source": [
    "In the two tables above we can see that statistics of the bad segments for both the original text and the Tesseract version, and we can clearly see that the Tesseract algorithm scores better on all aspects. Now we will do a bit more of a detailed analysis on the differences between the two versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b982a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 3))\n",
    "# Do the mean bad segment length first\n",
    "sns.kdeplot(pdftotext_results['Number of bad Segments'], ax=axes[0],  clip = (0.0, 100.0), label=\"original\", color='red')\n",
    "sns.kdeplot(tesseract_results['Number of bad Segments'], ax=axes[0],  clip = (0.0, 100.0), label='tesseract', color='blue', linestyle='--')\n",
    "\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel(\"Number of bad segments\", fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "sns.kdeplot(pdftotext_results['Number of bad Tokens'], ax=axes[1], clip = (0.0, 100.0), label=\"original\", color='red')\n",
    "sns.kdeplot(tesseract_results['Number of bad Tokens'], ax=axes[1], clip = (0.0, 100.0), label='tesseract', color='blue', linestyle='--')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel(\"Number of bad words\", fontsize=12)\n",
    "axes[1].legend()\n",
    "\n",
    "sns.kdeplot(pd.Series(all_seg_lengths_pdf), ax=axes[2],  clip = (0.0, 25.0), label=\"original\", color='red')\n",
    "sns.kdeplot(pd.Series(all_seg_lengths_tes), ax=axes[2],  clip = (0.0, 25.0), label='tesseract', color='blue', linestyle='--')\n",
    "axes[2].set_xlabel(\"bad segment length\", fontsize=12)\n",
    "axes[2].legend()\n",
    "\n",
    "kdeline1 = axes[0].lines[0]\n",
    "kdeline2 = axes[0].lines[1]\n",
    "kdeline3 = axes[1].lines[0]\n",
    "kdeline4 = axes[1].lines[1]\n",
    "kdeline5 = axes[2].lines[0]\n",
    "kdeline6 = axes[2].lines[1]\n",
    "\n",
    "xs1, ys1 = kdeline1.get_xdata(), kdeline1.get_ydata()\n",
    "xs2, ys2 = kdeline2.get_xdata(), kdeline2.get_ydata()\n",
    "xs3, ys3 = kdeline3.get_xdata(), kdeline3.get_ydata()\n",
    "xs4, ys4 = kdeline4.get_xdata(), kdeline4.get_ydata()\n",
    "xs5, ys5 = kdeline5.get_xdata(), kdeline5.get_ydata()\n",
    "xs6, ys6 = kdeline6.get_xdata(), kdeline6.get_ydata()\n",
    "\n",
    "mean1 = pdftotext_results['Number of bad Segments'].median()\n",
    "mean2 = tesseract_results['Number of bad Segments'].median()\n",
    "mean3 = pdftotext_results['Number of bad Tokens'].median()\n",
    "mean4 = tesseract_results['Number of bad Tokens'].median()\n",
    "mean5 = pd.Series(all_seg_lengths_pdf).median()\n",
    "mean6 = pd.Series(all_seg_lengths_tes).median()\n",
    "\n",
    "height1 = np.interp(mean1, xs1, ys1)\n",
    "height2 = np.interp(mean2, xs2, ys2)\n",
    "height3 = np.interp(mean3, xs3, ys3)\n",
    "height4 = np.interp(mean4, xs4, ys4)\n",
    "height5 = np.interp(mean5, xs5, ys5)\n",
    "height6 = np.interp(mean6, xs6, ys6)\n",
    "\n",
    "axes[0].vlines(mean1, 0, height1, color='red')\n",
    "axes[0].vlines(mean2, 0, height2, color='blue', ls=':')\n",
    "axes[1].vlines(mean3, 0, height3, color='red')\n",
    "axes[1].vlines(mean4, 0, height4, color='blue', ls=':')\n",
    "axes[2].vlines(mean5, 0, height5, color='red')\n",
    "axes[2].vlines(mean6, 0, height6, color='blue', ls=':')\n",
    "\n",
    "# Get the medians and put them im the figures\n",
    "axes[0].set_title(\"Median original=%.2f\\nMedian Tesseract=%.2f\" % (mean1, mean2), fontsize=10)\n",
    "axes[1].set_title(\"Median original=%.2f\\nMedian Tesseract=%.2f\" % (mean3, mean4), fontsize=10)\n",
    "axes[2].set_title(\"Median original=%.2f\\nMedian Tesseract=%.2f\" % (mean5, mean6), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf141b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For pdftotext if we sum the length of all bad segments we get %d bad tokens\" % sum(all_seg_lengths_pdf))\n",
    "print(\"For Tesseract if we sum the length of all bad segments we get %d bad tokens\" % sum(all_seg_lengths_tes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296109a",
   "metadata": {},
   "source": [
    "## Comparing Bad Segments\n",
    "\n",
    "Now that we have the bad segments for all the pages for both the original and the output from Tesseract, we can compare those bad segments and see where the two versions are different. For this we have to be a bit careful, as there is not always a 1 to 1 mapping from one version to the other, so we will only select the pages that have the same amount of tokens for both version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc411be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlap_info(list_of_indices_1, list_of_indices_2):\n",
    "    \"\"\"\n",
    "    Function to calculate the overlap between two sets of annotations for bad segments,\n",
    "    where both inputs should be lists with the starting indices of bad segments.\n",
    "    :param list_of_indices_1: list of indices for bad segments for system 1\n",
    "    :param list_of_indices_2: list of indices for bad segments for system 2\n",
    "    :returns: three output statistics for the overlap between segments\n",
    "        - no_overlap: percentage of clusters that have no overlap between system 1 and 2\n",
    "        - some_overlap: percentage of clusters where there is some overlap between system 1 and 2\n",
    "        but one is not a subset of the other.\n",
    "        - subsegment: percentage of clusters where segment 2 is a subset of segment 1.\n",
    "    \n",
    "    \"\"\"\n",
    "    # let's get the average overlap as IoU\n",
    "    no_overlap = 0\n",
    "    some_overlap = 0\n",
    "    subsegment = 0\n",
    "    sets_1 = [set(range(start, stop)) for start, stop in list_of_indices_1]\n",
    "    sets_2 = [set(range(start, stop)) for start, stop in list_of_indices_2]\n",
    "\n",
    "    for set_1 in sets_1:\n",
    "        if not any(set_1 & set_2 for set_2 in sets_2):\n",
    "            no_overlap+=1\n",
    "        elif any((set_1 & set_2 == set_1) for set_2 in sets_2):\n",
    "            subsegment+=1\n",
    "        else:\n",
    "            for set_2 in sets_2:\n",
    "                iou = len(set_1 & set_2) / len(set_1 | set_2)\n",
    "                if iou > 0.5:\n",
    "                    some_overlap+=1\n",
    "                    \n",
    "    return no_overlap, some_overlap, subsegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b091c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_pages = tesseract_results['Num Tokens'] == pdftotext_results['Num Tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_no_overlap = 0\n",
    "total_iou_vals = 0\n",
    "total_subsegment = 0\n",
    "\n",
    "for i in np.nonzero(np.array(matching_pages))[0]:\n",
    "    no_overlap, iou_vals, subsegment = get_overlap_info(tesseract_indices[i], pdftotext_indices[i])\n",
    "    total_no_overlap+=no_overlap\n",
    "    total_iou_vals+=iou_vals\n",
    "    total_subsegment+=subsegment\n",
    "    \n",
    "total_tesseract_segments = tesseract_results['Number of bad Segments'][matching_pages].sum()\n",
    "\n",
    "print(\"For tesseract, there is no overlap in %.2f percent of the cases\" % ((total_no_overlap/total_tesseract_segments)*100))\n",
    "print(\"For tesseract,  in %.2f percent of the cases the prediction is a subset of pdftotext\" % ((total_subsegment/total_tesseract_segments)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94849bbb",
   "metadata": {},
   "source": [
    "## Comparing across ministries\n",
    "\n",
    "Apart from comparing the statistics in general, we also compare the differences between the two versionss for the ministries seperately, to gain more insights into the differences between the two versions, and the potential impact of switching to Tesseract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdftotext_results.publisher.fillna('', inplace=True)\n",
    "tesseract_results.publisher.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot for the different ministeries\n",
    "pdftotext_results_ministeries = pdftotext_results[pdftotext_results.publisher.str.startswith('mnre')]\n",
    "tesseract_results_ministeries = tesseract_results[tesseract_results.publisher.str.startswith('mnre')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03fded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_dict = {\n",
    " 'mnre1109': 'OCW',\n",
    " 'mnre1010': 'AZ',\n",
    " 'mnre1013': 'BZ',\n",
    " 'mnre1018': 'DEF',\n",
    " 'mnre1025': 'VWS',\n",
    " 'mnre1034': 'BZK',\n",
    " 'mnre1045': 'EZK',\n",
    " 'mnre1058': 'J & V',\n",
    " 'mnre1073': 'SZW',\n",
    " 'mnre1090': 'FIN',\n",
    " 'mnre1130': 'I & W',\n",
    " 'mnre1153': 'LNV'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee19ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maak een plotje waarbij we alleen dat laatste figuur laten zien.\n",
    "df3 = pd.DataFrame({'original': pdftotext_results_ministeries.groupby('publisher')['Mean Bad Segment Length'].mean(),\n",
    "      'tesseract': tesseract_results_ministeries.groupby('publisher')['Mean Bad Segment Length'].mean()}).sort_values(by='original', ascending=False)\n",
    "df3.T.rename(translation_dict, axis=1).T.plot.barh()\n",
    "plt.xlabel('bad segment length', fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb42fd",
   "metadata": {},
   "source": [
    "This plot is very interesting, as it shows as that not only Tesseract has  much shorter lenghts of bad segments, the variance between the different ministries is also a lot lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4427d24",
   "metadata": {},
   "source": [
    "As we can see from both the plots and the describe tables above, the Tesseract method has higher quality OCR compared to pdftotext using this bad segment detection method. It also shows that on average (and definitely for Tesseract), the bad segment length is only a few words, which would make correction hopefully easier. However, it might also mean that when using chatgpt we might want to give the model some surrounding text to help, otherwise it would just be correction single wrong words without a clue of the context, causing it to make assumptions on general language statistics that might not hold for this very specific usecase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f51816",
   "metadata": {},
   "source": [
    "<h1><a id=\"chatgpt\">ChatGPT</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa77657f",
   "metadata": {},
   "source": [
    "Now that we have a method of detecting bad segments, let's try correcting them with chatGPT. We could also use the Davinci model, which is specifically trained for this kind of single turn tasks, performance should be about the same, but Davinci is more expensive to run.\n",
    "\n",
    "For this part I am interested in several types of mistakes: streams of concatenated words, short segments with minor mistakes and segments that are complete gibberish. Because especially for Tesseract the mean bad segment length is quite short (just a few tokens), I will mix up the long and short sentences so that we get a nice estimate on the performance of the correction step. Because some of the mistakes are irrecoverable, we try to combat this by adding to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a01a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the API, please replace this key below with your personal key\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"INSERT YOUR TOKEN HERE\"\n",
    "\n",
    "# small function to get the output of the ChatGPT model\n",
    "def get_response(prompt: str, model) -> str:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages, temperature=0.0)\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59322ebf",
   "metadata": {},
   "source": [
    "For the correction task a relative short precise prompt works well, I have modified it slightly so that it removes any markup (the triple backticks) that were in the original sentence. Below is a small summary of my iterative prompt design, and why I made certain adjustements to the prompt.\n",
    "\n",
    "We also write a short wrapper function to be able to just insert our sentences. Let's set up the boilerplate code and show an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172fd0d",
   "metadata": {},
   "source": [
    "### Prompt Design for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffde9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_prompt =\"\"\"\n",
    "Correct the spelling mistakes in the following Dutch text delimited by triple backticks and remove the triple backtiks afterwards.\n",
    "Leave the correct words untouched.\n",
    "```%s```\n",
    "\n",
    "\"\"\"\n",
    "# small lambda function to easily change the input sentences with the same prompt\n",
    "prompt_with_sentence = lambda sentence: correction_prompt % sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e0ea2",
   "metadata": {},
   "source": [
    "Here we write a function that extracts the text from the bad segments in pages, runs ChatGPT and saves the result. We then do the same calculations as we did before, run our bad segment detector and seeing how well this works.\n",
    "We end with a small manual evaluation of the corrected sentences as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3596e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_segment_detector = BadSegmentDetector()\n",
    "def correct_text_with_chatgpt(input_document: str, context_size: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    This function implements a document correction method using chatGPT, where we take as input\n",
    "    a page, detect bad segments, send these to chatgpt for correction, and place them back into the original text.\n",
    "    :param input_document: string that contains the input text with errors that we want to correct.\n",
    "    :param context_size: int specifying the number of words that we want to include as context for the \n",
    "    model to help it in correctiong shorter segments.\n",
    "    \n",
    "    \"\"\"\n",
    "    # The first step is to detect bad segments\n",
    "    detected_bad_segments = bad_segment_detector.detect_bad_segments(input_document, clean_input=False)\n",
    "    # Now we get the bad segments from the text, including a little bit of context in the response.\n",
    "    tokens = preprocess(input_document)\n",
    "    bad_indices = []\n",
    "    if detected_bad_segments[0]:\n",
    "        bad_seg_idx = [0]\n",
    "    else:\n",
    "        # no bad segments\n",
    "        bad_seg_idx = []\n",
    "    \n",
    "    # get a list of lists with the start and end indices in it\n",
    "    for i in range(1, len(detected_bad_segments)):\n",
    "        if [detected_bad_segments[i-1], detected_bad_segments[i]] == [False, True]:\n",
    "            bad_seg_idx.append(i)\n",
    "        elif [detected_bad_segments[i-1], detected_bad_segments[i]] == [True, False]:\n",
    "            bad_seg_idx.append(i-1)\n",
    "            bad_indices.append(bad_seg_idx)\n",
    "            bad_seg_idx = []\n",
    "    # clean up\n",
    "    if len(bad_seg_idx):\n",
    "        bad_seg_idx.append(len(detected_bad_segments)-1)\n",
    "    \n",
    "    if all(detected_bad_segments):\n",
    "        bad_indices = [[0, len(tokens)-1]]\n",
    "        \n",
    "    segment_pairs = []\n",
    "    corrected_segments = {}\n",
    "    # Now we can feed add some context and feed this to chat GPT\n",
    "    for start_idx, end_idx in bad_indices:\n",
    "        start_with_context = start_idx-context_size if start_idx-context_size >=0 else start_idx\n",
    "        end_with_context = end_idx+1+context_size if end_idx+1+context_size < len(tokens) else len(tokens)\n",
    "        input_text = \" \".join(tokens[start_with_context:end_with_context])\n",
    "        corrected_text = get_response(prompt_with_sentence(input_text), model=\"gpt-3.5-turbo-0301\")\n",
    "        corrected_text = \" \".join(corrected_text.split()[(start_idx-start_with_context): (start_idx-start_with_context)+(end_idx-start_idx)+1])\n",
    "        segment_pairs.append([\" \".join(tokens[start_idx:end_idx+1]), corrected_text])\n",
    "        corrected_segments[(start_idx, end_idx)] = corrected_text\n",
    "\n",
    "    # We have to be careful when placing things back in the original texts because we are working with lists of tokens\n",
    "    replaced_text = []\n",
    "    bad_segment_keys = list(corrected_segments.keys())\n",
    "    for i in range(len(tokens)):\n",
    "        in_segment = False\n",
    "        for bad_seg_key in bad_segment_keys:\n",
    "            if i == bad_seg_key[0]:\n",
    "                in_segment=True\n",
    "                replaced_text.append(corrected_segments[bad_seg_key])\n",
    "            elif i in range(bad_seg_key[0], bad_seg_key[1]+1):\n",
    "                in_segment = True\n",
    "        if not in_segment:\n",
    "            replaced_text.append(tokens[i])\n",
    "    \n",
    "    return{'text': \" \".join(replaced_text), 'pairs': segment_pairs}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb2c1d",
   "metadata": {},
   "source": [
    "Let's try and example of a very bad segment from the dataset and see what ChatGPT can do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_bad_segment = \"\"\"\n",
    " auo asewordig jesnyjng zo uy aul asebuny jo asoysiy y aaiuns of naa ul btuwioy uea jnapessegwuy eyuauy yd uea apessequy juauewsag assee3uoh uauewusg asseebu0h japsa1ds jba yooiaim uarsaprz ngonaeg yawyy sapwaig asyiny voueqi 9tozse uoueqn 9t02se uoueadn 9t0zs uovegr 9t02se puepapan puejapin 9t0es6t puejsapan 9tozdet puejsyng 9tijde puersying 9toleot pueisiing 9tote9t ¢ stozsot pubhspan otozest puehapin 9tozsot puepeppenn stozsb puejjapan sp purlbpin 9tozb ueiiked 9tozsot puepsapan 9t0z0l puepapan 9tozsot puejsapan 9tozvsz enawy jtozvzz puejapan stozsot ppuepapan stotsot puejlapann 9tozsot puejsapan 9tozviz puepapann 9tozpbz puejsapan 9t0tv8t afanl 9totete pywouy 9tozete eyuawiy 9t02ete puepapay puepapan ppuejsapan otozest puelapan 9toz epawy 9toze puelipin otoze puchopsn otoz pueejjapann 9tozb ajsaunl otozez puclopon otozlt afsesuoy 9to72t afue8uoy ppueapan 9toz puejsapan 9tozzzz,\n",
    "\"\"\"\n",
    "correction_prompt_original =\"\"\"\n",
    "Correct the following Dutch text containing spelling mistakes delimited by triple backticks and remove the triple backticks afterwards.\n",
    "Leave the correct words untouched. '\n",
    "```{%s}```\n",
    "\n",
    "\"\"\"\n",
    "prompt_with_sentence = lambda sentence: correction_prompt_original % sentence\n",
    "corrected_sentence_original = get_response(prompt_with_sentence(very_bad_segment), model=\"gpt-3.5-turbo-0301\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_bad_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_sentence_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d7ad5",
   "metadata": {},
   "source": [
    "Here we can see the danger: the original text is of too low quality, and should be sent back for correction. However, chatgpt will happily go ahead on generate a grammatical sentence, which is completely wrong in this context. There are two options: Either we automatically reject segments that are longer than a certain size, or we can see if we can get chatgpt to filter this out itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e8586f",
   "metadata": {},
   "source": [
    "One of the issues that we can have with ChatGPT is that, in general, it will be able to repair non-words and convert them to correct dutch words. However, this does of course not have to be the correct word with regards to the original text. For this, we compare two more things: whether the number of tokens returned by ChatGPT is equal to the number of tokens in the original text, and, if this is the case, what the Levenshtein distance between the tokens is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a6e40",
   "metadata": {},
   "source": [
    "## Detailed Evaluation of ChatGPT output\n",
    "\n",
    "For this we load the converted pages from our saved dataframe (i will first combine them), then we will compare them to the tesseract text with the bad segment statistics, and check out the token level distances.\n",
    "\n",
    "We will select pages that are relatively bad, so that there are some things to correct, we take the pages sorted on the number of bad tokens, and randomly sample pages between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select the samples\n",
    "selected_besluiten = pdftotext_results.sample(100)\n",
    "selected_ids = selected_besluiten['ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1582be",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_input = experiment_dataframe[experiment_dataframe['ID'].isin(selected_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3edd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 100 bad pages\n",
    "from tqdm import tqdm\n",
    "def get_chatgpt_for_pages(dataframe, text_type: str = 'tesseract'):\n",
    "    missed_pages = []\n",
    "    all_token_pairs = {}\n",
    "    all_corrected_texts = {}\n",
    "    for _, page in tqdm(dataframe.iterrows()):\n",
    "        try:\n",
    "            input_text = page[text_type]\n",
    "            page_id = page['ID']\n",
    "            chatgpt_output = correct_text_with_chatgpt(input_text)\n",
    "\n",
    "            all_token_pairs[page_id] = chatgpt_output['pairs']\n",
    "            all_corrected_texts[page_id] = chatgpt_output['text'] \n",
    "        except openai.error.RateLimitError as r:\n",
    "            missed_pages.append(page_id)\n",
    "        except openai.error.APIError as a:\n",
    "            missed_pages.append(page_id)\n",
    "    return all_token_pairs, all_corrected_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc717ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdftotext_chatgpt_token_pairs, pdftotext_chatgpt_text = get_chatgpt_for_pages(chatgpt_input, text_type='original')\n",
    "tesseract_chatgpt_token_pairs, tesseract_chatgpt_text = get_chatgpt_for_pages(chatgpt_input, text_type='teseract')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b4cab",
   "metadata": {},
   "source": [
    "As the main goal here is to compare to the texts without ChatGPT we get the stats of the versions without ChatGPT of the selected sample, so that we can compare it to the versions we corrected with ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdftotext_results[pdftotext_results['ID'].isin(selected_ids)].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97221d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseract_results[tesseract_results['ID'].isin(selected_ids)].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e59c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chatgpt_results.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d9dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_chatgpt_results.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf366b1",
   "metadata": {},
   "source": [
    "## PDF Layout Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648c5e0",
   "metadata": {},
   "source": [
    "The other part of this research is validating and repairing the PDF layout by converting it to markdown. For this we will use the veraPDF software to check our PDF, and `pdf2html` to convert the PDF to the markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c1b63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we write the code to check a pdf with verapdf\n",
    "import os\n",
    "import lxml.etree as et\n",
    "\n",
    "def run_verapdf(input_pdf_path: str):\n",
    "    \"\"\"\n",
    "    This function will run verapdf on the input file and create an output xml file in the \n",
    "    same folder as the input containing the result of the report.\n",
    "    \"\"\"\n",
    "    # Run VeraPDF\n",
    "    stream = os.popen('/Users/rubenvanheusden/verapdf/verapdf --profile /Users/rubenvanheusden/verapdf/profiles/veraPDF-validation-profiles-rel-1.24/PDF_UA/WCAG-21-Complete.xml --format xml '+ input_pdf_path)\n",
    "    # Read the output\n",
    "    verapdf_output = stream.read().encode()\n",
    "    xml_output = et.fromstring(verapdf_output)\n",
    "    \n",
    "    # Save VeraPDF xml results a file\n",
    "    output_path = input_pdf_path.replace('.pdf', '.xml')\n",
    "    output_tree = et.ElementTree(xml_output)\n",
    "    output_tree.write(output_path, pretty_print=True)\n",
    "    \n",
    "    return et.tostring(output_tree, pretty_print=True).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1064d117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<report>\n",
      "  <buildInformation>\n",
      "    <releaseDetails id=\"core\" version=\"1.24.1\" buildDate=\"2023-06-22T10:38:00+02:00\"/>\n",
      "    <releaseDetails id=\"validation-model\" version=\"1.24.1\" buildDate=\"2023-06-22T11:37:00+02:00\"/>\n",
      "    <releaseDetails id=\"gui\" version=\"1.24.1\" buildDate=\"2023-06-22T14:19:00+02:00\"/>\n",
      "  </buildInformation>\n",
      "  <jobs>\n",
      "    <job>\n",
      "      <item size=\"272073\">\n",
      "        <name>/Users/rubenvanheusden/Desktop/TPDL/codebases/accessibilifier/notebooks/../examples/Woo-verzoek_Geredigeerd.pdf</name>\n",
      "      </item>\n",
      "      <validationReport jobEndStatus=\"normal\" profileName=\"WCAG-2.1-Complete validation profile\" statement=\"PDF file is not compliant with Validation Profile requirements.\" isCompliant=\"false\">\n",
      "        <details passedRules=\"155\" failedRules=\"5\" passedChecks=\"188\" failedChecks=\"15\">\n",
      "          <rule specification=\"ISO 14289-1:2014\" clause=\"7.1\" testNumber=\"10\" status=\"failed\" passedChecks=\"0\" failedChecks=\"1\">\n",
      "            <description>The document catalog dictionary shall include a ViewerPreferences dictionary containing a DisplayDocTitle key, whose value shall be true.</description>\n",
      "            <object>CosDocument</object>\n",
      "            <test>DisplayDocTitle == true</test>\n",
      "            <check status=\"failed\">\n",
      "              <context>root</context>\n",
      "              <errorMessage>ViewerPreferences dictionary is not present in the document Catalog, or DisplayDocTitle key is set to false or is not present in the ViewerPreferences dictionary (ViewerPreferences = null, DisplayDocTitle = null)</errorMessage>\n",
      "            </check>\n",
      "          </rule>\n",
      "          <rule specification=\"ISO 14289-1:2014\" clause=\"7.2\" testNumber=\"34\" status=\"failed\" passedChecks=\"0\" failedChecks=\"2\">\n",
      "            <description>Natural language for text in page content shall be determined</description>\n",
      "            <object>SETextItem</object>\n",
      "            <test>gSizeCatalogLang != 0 || Lang != null</test>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[0](29 0 obj PDPage)/contentStream[0](31 0 obj PDSemanticContentStream)/content[1]{mcid:0}/contentItem[0]{mcid:0}</context>\n",
      "              <errorMessage>Natural language for text in page content cannot be determined</errorMessage>\n",
      "            </check>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[1](1 0 obj PDPage)/contentStream[0](2 0 obj PDSemanticContentStream)/content[1]{mcid:0}/contentItem[0]{mcid:0}</context>\n",
      "              <errorMessage>Natural language for text in page content cannot be determined</errorMessage>\n",
      "            </check>\n",
      "          </rule>\n",
      "          <rule specification=\"ISO 14289-1:2014\" clause=\"7.1\" testNumber=\"9\" status=\"failed\" passedChecks=\"0\" failedChecks=\"1\">\n",
      "            <description>The Metadata stream in the document's catalog dictionary shall contain a dc:title entry, where dc is the recommended prefix for the Dublin Core metadata schema as defined in the XMP specification, which clearly identifies the document.</description>\n",
      "            <object>MainXMPPackage</object>\n",
      "            <test>dc_title != null &amp;&amp; dc_title != ''</test>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/metadata[0](10 0 obj PDMetadata)/XMPPackage[0]</context>\n",
      "              <errorMessage>Metadata stream does not contain dc:title</errorMessage>\n",
      "            </check>\n",
      "          </rule>\n",
      "          <rule specification=\"ISO 14289-1:2014\" clause=\"7.1\" testNumber=\"3\" status=\"failed\" passedChecks=\"0\" failedChecks=\"10\">\n",
      "            <description>Content shall be marked as Artifact or tagged as real content</description>\n",
      "            <object>SESimpleContentItem</object>\n",
      "            <test>parentStructureTag != null || parentsTags.split('&amp;').filter(elem =&gt; elem == 'Artifact').length &gt; 0</test>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[0](29 0 obj PDPage)/contentStream[0](31 0 obj PDSemanticContentStream)/content[2]/contentItem[0]</context>\n",
      "              <errorMessage>Content is neither marked as Artifact nor tagged as real content</errorMessage>\n",
      "            </check>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[0](29 0 obj PDPage)/contentStream[0](31 0 obj PDSemanticContentStream)/operators[32]/xObject[0]/contentStream[0](35 0 obj PDSemanticContentStream)/content[0]/contentItem[0]</context>\n",
      "              <errorMessage>Content is neither marked as Artifact nor tagged as real content</errorMessage>\n",
      "            </check>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[0](29 0 obj PDPage)/contentStream[0](31 0 obj PDSemanticContentStream)/operators[32]/xObject[0]/contentStream[0](35 0 obj PDSemanticContentStream)/content[0]/contentItem[1]</context>\n",
      "              <errorMessage>Content is neither marked as Artifact nor tagged as real content</errorMessage>\n",
      "            </check>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[0](29 0 obj PDPage)/contentStream[0](31 0 obj PDSemanticContentStream)/operators[38]/xObject[0]/contentStream[0](36 0 obj PDSemanticContentStream)/content[0]/contentItem[0]</context>\n",
      "              <errorMessage>Content is neither marked as Artifact nor tagged as real content</errorMessage>\n",
      "            </check>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[0](29 0 obj PDPage)/contentStream[0](31 0 obj PDSemanticContentStream)/operators[43]/xObject[0]/contentStream[0](37 0 obj PDSemanticContentStream)/content[0]/contentItem[0]</context>\n",
      "              <errorMessage>Content is neither marked as Artifact nor tagged as real content</errorMessage>\n",
      "            </check>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[1](1 0 obj PDPage)/contentStream[0](2 0 obj PDSemanticContentStream)/content[2]/contentItem[0]</context>\n",
      "              <errorMessage>Content is neither marked as Artifact nor tagged as real content</errorMessage>\n",
      "            </check>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[1](1 0 obj PDPage)/contentStream[0](2 0 obj PDSemanticContentStream)/operators[32]/xObject[0]/contentStream[0](5 0 obj PDSemanticContentStream)/content[0]/contentItem[0]</context>\n",
      "              <errorMessage>Content is neither marked as Artifact nor tagged as real content</errorMessage>\n",
      "            </check>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[1](1 0 obj PDPage)/contentStream[0](2 0 obj PDSemanticContentStream)/operators[32]/xObject[0]/contentStream[0](5 0 obj PDSemanticContentStream)/content[0]/contentItem[1]</context>\n",
      "              <errorMessage>Content is neither marked as Artifact nor tagged as real content</errorMessage>\n",
      "            </check>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[1](1 0 obj PDPage)/contentStream[0](2 0 obj PDSemanticContentStream)/operators[38]/xObject[0]/contentStream[0](6 0 obj PDSemanticContentStream)/content[0]/contentItem[0]</context>\n",
      "              <errorMessage>Content is neither marked as Artifact nor tagged as real content</errorMessage>\n",
      "            </check>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/pages[1](1 0 obj PDPage)/contentStream[0](2 0 obj PDSemanticContentStream)/operators[43]/xObject[0]/contentStream[0](7 0 obj PDSemanticContentStream)/content[0]/contentItem[0]</context>\n",
      "              <errorMessage>Content is neither marked as Artifact nor tagged as real content</errorMessage>\n",
      "            </check>\n",
      "          </rule>\n",
      "          <rule specification=\"ISO 14289-1:2014\" clause=\"5\" testNumber=\"1\" status=\"failed\" passedChecks=\"0\" failedChecks=\"1\">\n",
      "            <description>The PDF/UA version and conformance level of a file shall be specified using the PDF/UA Identification extension schema.</description>\n",
      "            <object>MainXMPPackage</object>\n",
      "            <test>UAIdentification_size == 1</test>\n",
      "            <check status=\"failed\">\n",
      "              <context>root/document[0]/metadata[0](10 0 obj PDMetadata)/XMPPackage[0]</context>\n",
      "              <errorMessage>The document metadata stream doesn't contains PDF/UA Identification Schema.</errorMessage>\n",
      "            </check>\n",
      "          </rule>\n",
      "        </details>\n",
      "      </validationReport>\n",
      "      <duration start=\"1689857226115\" finish=\"1689857226552\">00:00:00.437</duration>\n",
      "    </job>\n",
      "  </jobs>\n",
      "  <batchSummary totalJobs=\"1\" failedToParse=\"0\" encrypted=\"0\" outOfMemory=\"0\" veraExceptions=\"0\">\n",
      "    <validationReports compliant=\"0\" nonCompliant=\"1\" failedJobs=\"0\">1</validationReports>\n",
      "    <featureReports failedJobs=\"0\">0</featureReports>\n",
      "    <repairReports failedJobs=\"0\">0</repairReports>\n",
      "    <duration start=\"1689857226036\" finish=\"1689857226580\">00:00:00.544</duration>\n",
      "  </batchSummary>\n",
      "</report>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(run_verapdf('../examples/Woo-verzoek_Geredigeerd.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc2e5b",
   "metadata": {},
   "source": [
    "As we can see in the above code, VeraPDF produces a detailed XML report on the pdf file and the errors that it has. Our next step is to extract these errors from the XML output so that we can use to get a detailed report on the problems that the PDF file has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mdutils"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_redaction_env",
   "language": "python",
   "name": "text_redaction_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
